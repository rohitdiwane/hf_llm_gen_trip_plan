{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "1hmA__DrYy-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ib62rlgPX_sE"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = 'tiiuae/falcon-7b'\n",
        "hf_api_key = ''\n",
        "llm = HuggingFaceHub(huggingfacehub_api_token=hf_api_key,\n",
        "                     repo_id=repo_id,\n",
        "                     model_kwargs={\"temperature\": 0.7, \"max_new_tokens\": 500})"
      ],
      "metadata": {
        "id": "xj6PvzlWYNgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"\"\"Question: {question}\n",
        "            Answer: Let's give a detailed answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])"
      ],
      "metadata": {
        "id": "1ZYnNkaNYNjf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "pmU2Y8wcYNmn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain"
      ],
      "metadata": {
        "id": "x7Ofa7gZYNsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = chain.run(\"Give me a good trip plan for 5 days in Bangalore.\")\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m3_brxfYNvs",
        "outputId": "a8f682e0-6bce-43b4-a86c-6d7cbd3ea220"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. You can have a good plan (tour) for 5 days in bangaluru.\n",
            "2. First, visit Lalbagh botanical garden, the famous garden in South India.\n",
            "3. Then visit the Hidimbi hills.\n",
            "4. Next is the Kirloskar planetarium.\n",
            "5. Don't miss the historical and cultural places.\n",
            "6. Lastly, visit the Bangalore Palace.\n",
            "7. Don't forget to visit the boat house.\n",
            "8. And enjoy the city of bangaluru.\n",
            "\n",
            "One can have a good trip planning for 5 days in bangaluru.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "t216Keg7cWb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typing_extensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHxdYfIJcrLO",
        "outputId": "e9621143-8a7f-4055-e80c-c325b59ca689"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def generate_trip_plan(question):\n",
        "  hf_api_key = ''\n",
        "  repo_id = 'tiiuae/falcon-7b'\n",
        "  llm = HuggingFaceHub(huggingfacehub_api_token=hf_api_key,\n",
        "                     repo_id=repo_id,\n",
        "                     model_kwargs={\"temperature\": 0.7, \"max_new_tokens\": 500})\n",
        "  template=\"\"\"Question: {question}\n",
        "            Answer: Let's give a detailed answer.\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=['question'])\n",
        "  chain = LLMChain(prompt=prompt, llm=llm)\n",
        "  out = chain.run(question)\n",
        "  return out\n",
        "\n",
        "input_text = gr.Textbox(label=\"Enter your question\")\n",
        "generate_button = gr.Button(\"Generate\")\n",
        "\n",
        "iface = gr.Interface(fn=generate_trip_plan, inputs=input_text, outputs=gr.Textbox())\n",
        "iface.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "tpspT3KVkqZx",
        "outputId": "12154cb5-2321-4bb2-e51a-1d3d83932e54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a270024839a3579133.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a270024839a3579133.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7864 <> https://a270024839a3579133.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CnGjYD3xYOJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CObyt6v6YOMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXWuAgvXYORi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYUgTXLiYOUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUK6WS71YOXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}